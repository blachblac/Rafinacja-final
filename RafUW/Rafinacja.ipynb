{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bag of Words (BoW)\n",
    "- **Czym jest model BoW i dlaczego jest potrzebny do reprezentacji tekstu?**\n",
    "    \n",
    "    Model BoW to technika reprezentacji tekstu, w której tekst jest rozdzielany na pojedyncze słowa, a następnie każde słowo jest traktowane jako pojedyncza jednostka, bez uwzględnienia jego kolejności i kontekstu.\n",
    "        \n",
    "    W modelu Bag of Words każde słowo w tekście jest reprezentowane przez unikalny identyfikator liczbowy. Dzięki temu modelowi, możemy przetwarzać tekst w sposób numeryczny i wykorzystywać go w algorytmach uczenia maszynowego. Wadą modelu BoW jest brak uwzględnienia kontekstu i sekwencji słów, co może prowadzić do utraty pewnej ilości informacji zawartej w tekście.\n",
    "        \n",
    "    Model BoW jest przydatny w analizie tekstu, ponieważ pozwala na łatwe i efektywne przetwarzanie dużych zbiorów danych tekstowych. Może być stosowany w różnych zastosowaniach, takich jak analiza sentymentu, klasyfikacja tekstu czy generowanie rekomendacji.\n",
    "    \n",
    "- **Czym jest model BoW i dlaczego jest potrzebny do reprezentacji tekstu?Jak opracować model Bag of Words dla zbioru dokumentów?**\n",
    "    1. Przygotowanie danych: Przed przystąpieniem do tworzenia modelu, należy przygotować zbiór danych, który będzie składał się z dokumentów tekstowych. W tym kroku, można zastosować techniki preprocessingu, takie jak usuwanie znaków interpunkcyjnych, stop words, normalizacja tekstu, itp.\n",
    "\n",
    "    2. Stworzenie słownika: Następnie, trzeba stworzyć słownik wszystkich słów występujących w zbiorze dokumentów. Każde słowo w słowniku otrzymuje unikalny identyfikator liczbowy.\n",
    "\n",
    "    3. Przygotowanie reprezentacji Bag of Words: Na podstawie słownika, tworzy się reprezentację Bag of Words dla każdego dokumentu. W reprezentacji tej, dla każdego słowa zdefiniowanego w słowniku, tworzona jest kolumna, a dla każdego dokumentu tworzony jest wiersz. W komórce w kolumnie i wierszu, odpowiadającej danemu słowu i danemu dokumentowi, umieszcza się liczbę wystąpień tego słowa w danym dokumencie.\n",
    "\n",
    "    4. Zastosowanie modelu\n",
    "    \n",
    "- **Jak korzystać z różnych technik w celu przygotowania słownictwa i punktacji słów?**\n",
    "    - ignorowanie: wersalików, interpunkcji, zaimki przyimki liczebniki - stop words, lematyzacja\n",
    "    - grupowanie: generowanie z tekstu tokenów więcej niż jednoskładnikowych - np dwuskładnikowy token = bigram, czyli używanie sekwencji słów jako tokenu\n",
    "    - ocenianie: ocenianie występowania tokenu w danym dokumencie na zasadzie:\n",
    "            - licznika - częstość występowania słowa w dokumencie (n = x*token)\n",
    "        - częstotliwości - częstotliwość z jaką token pojawia się w tekście spośród wszystkich słów (token/n-słów_w_dokumencie)\n",
    "            - Term Frequency\n",
    "            - Inverse Document Frequency\n",
    "    - Hashing: nadawanie tokenowi przedstawienia numerycznego, w przypadku duzych korpusów ma to sens ze względu na możliwość manipulacji hashowaniem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Przykład BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are',\n",
       " 'bad',\n",
       " 'be',\n",
       " 'cant',\n",
       " 'contantly',\n",
       " 'crashes',\n",
       " 'fix',\n",
       " 'fixes',\n",
       " 'for',\n",
       " 'game',\n",
       " 'issues',\n",
       " 'it',\n",
       " 'make',\n",
       " 'overlooked',\n",
       " 'performance',\n",
       " 'please',\n",
       " 'this',\n",
       " 'unplayable',\n",
       " 'wait']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"Fix this game please\", \"Wait for performance fixes\", \"Performance Issues cant be overlooked\", \"Crashes make it unplayable\", \"Performance issues are bad\", \"Game crashes contantly\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(text)\n",
    "\n",
    "(sorted(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 19)\n",
      "[[0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1]\n",
      " [0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vector = vectorizer.transform(text)\n",
    "\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF na tym samym korpusie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'bad', 'be', 'cant', 'contantly', 'crashes', 'fix', 'fixes', 'for', 'game', 'issues', 'it', 'make', 'overlooked', 'performance', 'please', 'this', 'unplayable', 'wait']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(text)\n",
    "\n",
    "print(sorted(vectorizer.vocabulary_))\n",
    "vector = vectorizer.transform([text[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.25276297, 2.25276297, 2.25276297, 2.25276297, 2.25276297,\n",
       "       1.84729786, 2.25276297, 2.25276297, 2.25276297, 1.84729786,\n",
       "       1.84729786, 2.25276297, 2.25276297, 2.25276297, 1.55961579,\n",
       "       2.25276297, 2.25276297, 2.25276297, 2.25276297])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.idf_ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Modele BoW moga byc wykorzystywane np. przy klasyfikacji komentarzy w celu automatycznego usuwania/banowania użytkowników"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word Embeddings (WE)\n",
    "\n",
    "- **Czym jest metoda WE w reprezentacji tekstu i czym różni się ona od innych metod?**\n",
    "    Metoda Word Embeddings to technika reprezentacji tekstu w postaci wektorów liczb rzeczywistych, które opisują semantyczne znaczenie słów w kontekście tekstu. W przeciwieństwie do modelu Bag of Words, metoda Word Embeddings uwzględnia kontekst i sekwencję słów w tekście, co pozwala na bardziej dokładne reprezentowanie znaczenia słów.\n",
    "\n",
    "- **Główne algorytmy osadzania słów z danych tekstowych**\n",
    "    1. Word2Vec: Word2Vec to algorytm opierający się on na trenowaniu sieci neuronowej, w celu reprezentacji słów w postaci wektorów, tak aby słowa, które często występują razem w kontekście, miały podobne reprezentacje wektorowe\n",
    "        - Continous Bag of Words (CBoW): przewidywanie słów na podstawie kontekstu sąsiadujących słów\n",
    "        - SkipGram: przewidywanie kontekstu na podstawie par słów\n",
    "    2. GloVe (GlobalVectors): Algorytm oparty na liniowej redukcji wymiarów, który pozwala na uzyskanie reprezentacji słów w postaci wektorów o mniejszej liczbie wymiarów, tak aby słowa, które są często używane razem, były blisko siebie w przestrzeni wektorowej\n",
    "    3. FastText: podobny do Word2Vec ale oparty na n-gramach a nie pojedynczych tokenach\n",
    "\n",
    "- **Techniki ekstrakcji cech - techniki pozwalające na efektywne wykorzystanie danych w machine learningu**\n",
    "    w przypadku NLM polegają na zamianie tekstu na cyfry - wektory, a różne techniki podchodzą w inny sposób do przedstawienia dokumentu/słowa np w formie par, ilości w dokumencie/korpusie itd\n",
    "        - one hot encoding - przypisanie dokumentowi ciągu zer i jedynek w postaci tabeli\n",
    "            #reszta przedstawiona w jakiś sposób wyżej w punkcie 2 lub 1#\n",
    "        - bag of words\n",
    "        - bag of n-grams\n",
    "        - Term frequency \n",
    "        - Inverse Document frequency\n",
    "        - word2vec\n",
    "\n",
    "- **Jak wytrenować nowe osadzenie, albo uzyć wytrenowane osadzenia w zadaniach przetwarzania języka:**\n",
    "\n",
    "    trenowanie osadzenia:\n",
    "        do wytrenowania nowego osadzenia wykrozystuje się jeden z algorytmów na dużych danych tekstowych w celu trenowania sieci neuronowej do wyznaczenie wektorowej reprezentacji słów\n",
    "        Proces:\n",
    "            1. przygotowanie danych:\n",
    "                - tokenizacja\n",
    "                - usunięcie stop words\n",
    "                - stemming itd\n",
    "            2. wybór algorytmu i hiperparametrów (rozmiary wektrów osadzenia, wartości minimalnej częstości, epoki itp; można do tego wykorzystać optymalizacje)\n",
    "            3. trenowanie modelu na danych tekstowych\n",
    "            4. ewaluacja modelu\n",
    "\n",
    "    reuse:\n",
    "        - statycznie - używanie osadzeń udostępnianych np. word2vec albo GloVe jako komponentów modelu w formie takiej jakiej już istnieją\n",
    "        - updated - gdzie osadzenie jest wykorzystywane jako podstawa i wykorzystywane razem z nowymi osadzeniami w trakcie trenowania modelu\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Przykład Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sjkli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sjkli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt') #do tokenizacji zdań\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fix', 'this', 'game', 'please'],\n",
       " ['wait', 'for', 'performance', 'fixes'],\n",
       " ['performance', 'issues', 'cant', 'be', 'overlooked'],\n",
       " ['crashes', 'make', 'it', 'unplayable'],\n",
       " ['performance', 'issues', 'are', 'bad'],\n",
       " ['game', 'crashes', 'contantly']]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lista list dla inputu do wrod2vec\n",
    "lower_text = [sentence.lower() for sentence in text]\n",
    "\n",
    "dokumenty = []\n",
    "\n",
    "for dokument in lower_text:\n",
    "    tokens = word_tokenize(dokument)\n",
    "    dokumenty.append(tokens)\n",
    "\n",
    "dokumenty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozmiar wektora słów: 100\n",
      "Liczba słów w korpusie: 6\n",
      "Liczba unikalnych słów: 19\n",
      "Lista unikalnych słów: ['performance', 'issues', 'game', 'crashes', 'contantly', 'this', 'please', 'wait', 'for', 'fixes', 'cant', 'bad', 'be', 'overlooked', 'make', 'it', 'unplayable', 'are', 'fix']\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(dokumenty, min_count=1, vector_size=100)\n",
    "\n",
    "#podsumowanie modelu\n",
    "print('Rozmiar wektora słów:', model.vector_size)\n",
    "print('Liczba słów w korpusie:', model.corpus_count)\n",
    "print('Liczba unikalnych słów:', len(model.wv.key_to_index))\n",
    "print('Lista unikalnych słów:', list(model.wv.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000535</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.005104</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>-0.009303</td>\n",
       "      <td>-0.007117</td>\n",
       "      <td>0.006459</td>\n",
       "      <td>0.008974</td>\n",
       "      <td>-0.005016</td>\n",
       "      <td>-0.003764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.00019</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.009619</td>\n",
       "      <td>0.00506</td>\n",
       "      <td>-0.008918</td>\n",
       "      <td>-0.007042</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.006392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.000535  0.000237  0.005104  0.009009 -0.009303 -0.007117  0.006459   \n",
       "\n",
       "         7         8         9   ...        90       91        92        93  \\\n",
       "0  0.008974 -0.005016 -0.003764  ...  0.001632  0.00019  0.003473  0.000218   \n",
       "\n",
       "         94       95        96        97        98        99  \n",
       "0  0.009619  0.00506 -0.008918 -0.007042  0.000901  0.006392  \n",
       "\n",
       "[1 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wektor dla konkretnego słowa\n",
    "display(pd.DataFrame(model.wv['performance']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zapis modelu\n",
    "model.save('model.bin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
